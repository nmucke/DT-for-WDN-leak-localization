training_params:
  epoch_count: 20
  batch_size: 128
  patience: 50

model_architecture: 'transformer'

model_type: 'WAE'

model_args:
  encoder:
    num_layers: 2
  decoder:
    num_layers: 2

optimizer_type: 'Adam'

optimizer_args:
  encoder:
    lr: 1e-2
  decoder:
    lr: 1e-2

data_params:
  num_pipes: 34
  num_nodes: 31
  num_time_steps: 24
